{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "导入所需库\n",
    "'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d40fca",
   "metadata": {},
   "source": [
    "# Labeling and Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 读取主数据和锚点文件\n",
    "df = pd.read_feather('train_data.feather')\n",
    "anchors = pd.read_feather('trading_days_anchor_1993_2000.feather')\n",
    "\n",
    "# 2. 合并锚点信息\n",
    "df = df.merge(\n",
    "    anchors[['date', 'anchor_5', 'anchor_20', 'anchor_60']],\n",
    "    on=['date'],\n",
    "    how='left'\n",
    ").sort_values(['id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# 3. 定义每种窗口的配置：窗口长度 / 锚点列 / 输出路径 / 输出标签文件\n",
    "configs = [\n",
    "    (5,  'anchor_5',  './charts_train/5d_charts',  './labels_train/image_labels_i5.feather'),\n",
    "    (20, 'anchor_20', './charts_train/20d_charts', './labels_train/image_labels_i20.feather'),\n",
    "    (60, 'anchor_60', './charts_train/60d_charts', './labels_train/image_labels_i60.feather'),\n",
    "]\n",
    "\n",
    "for window, anchor_col, img_dir, out_feather in configs:\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(out_feather), exist_ok=True)\n",
    "\n",
    "    image_labels = []\n",
    "\n",
    "    for id_val, grp in df.groupby('id'):\n",
    "        grp = grp.reset_index(drop=True)\n",
    "        # 找到所有锚点行索引\n",
    "        idxs = grp.index[grp[anchor_col] == 1.0].tolist()\n",
    "\n",
    "        for idx in idxs:\n",
    "            # 前面必须有 window-1 条数据\n",
    "            if idx >= window - 1:\n",
    "                win = grp.iloc[idx - (window - 1): idx + 1]\n",
    "\n",
    "                # 计算三个 horizon 的 label：看窗口末期的 ret_Xd 是否 > 0\n",
    "                label_5  = int(win['ret_5d'].iloc[-1]  > 0)\n",
    "                label_20 = int(win['ret_20d'].iloc[-1] > 0)\n",
    "                label_60 = int(win['ret_60d'].iloc[-1] > 0)\n",
    "\n",
    "                # 锚点日期（窗口最后一天）\n",
    "                anchor_date = pd.to_datetime(win['date'].iloc[-1]).strftime('%Y%m%d')\n",
    "\n",
    "                # 对应的图像路径\n",
    "                image_path = os.path.join(img_dir, f'id_{id_val}_{anchor_date}.png')\n",
    "\n",
    "                if os.path.exists(image_path):\n",
    "                    image_labels.append({\n",
    "                        'image_path': image_path,\n",
    "                        'id':         id_val,\n",
    "                        'date':       win['date'].iloc[-1],\n",
    "                        'label_5':    label_5,\n",
    "                        'label_20':   label_20,\n",
    "                        'label_60':   label_60,\n",
    "                    })\n",
    "\n",
    "    # 保存标签\n",
    "    labels_df = pd.DataFrame(image_labels)\n",
    "    labels_df.to_feather(out_feather)\n",
    "    print(f\"{window}d 图像标签已生成并保存到 {out_feather}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2263142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_and_split(labels_file, label_column, train_file, test_file):\n",
    "    # 读取标签文件\n",
    "    labels_df = pd.read_feather(labels_file)\n",
    "\n",
    "    # 获取标签为 0 和 1 的数据\n",
    "    label_0 = labels_df[labels_df[label_column] == 0]\n",
    "    label_1 = labels_df[labels_df[label_column] == 1]\n",
    "\n",
    "    # 确保选择相同数量的标签为 0 和 1 的样本\n",
    "    num_samples = min(len(label_0), len(label_1))\n",
    "    label_0 = label_0.sample(n=num_samples, random_state=42)\n",
    "    label_1 = label_1.sample(n=num_samples, random_state=42)\n",
    "\n",
    "    # 合并平衡后的数据并打乱\n",
    "    balanced_df = pd.concat([label_0, label_1]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # 按照 70% 训练集和 30% 测试集划分\n",
    "    train_df, test_df = train_test_split(\n",
    "        balanced_df, train_size=0.7, stratify=balanced_df[label_column], random_state=42\n",
    "    )\n",
    "\n",
    "    # 确保训练集和测试集标签分布均衡\n",
    "    print(f\"训练集标签分布 ({label_column}):\")\n",
    "    print(train_df[label_column].value_counts())\n",
    "    print(f\"\\n测试集标签分布 ({label_column}):\")\n",
    "    print(test_df[label_column].value_counts())\n",
    "\n",
    "    # 保存训练集和测试集到 Feather 文件\n",
    "    train_df.to_feather(train_file)\n",
    "    test_df.to_feather(test_file)\n",
    "\n",
    "    print(f\"数据集 {label_column} 已划分并保存！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [5, 20, 60]\n",
    "labels = [5, 20, 60]\n",
    "\n",
    "for h in horizons:\n",
    "    for l in labels:\n",
    "        balance_and_split(\n",
    "            labels_file = f'./labels_train/image_labels_i{h}.feather',\n",
    "            label_column = f'label_{l}',\n",
    "            train_file = f'./labels_train/train_labels_i{h}r{l}.feather',\n",
    "            test_file  = f'./labels_train/test_labels_i{h}r{l}.feather',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc27559",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备：{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39852a6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c853ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "定义 CNN\n",
    "'''\n",
    "\n",
    "class CNN5DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN5DModel, self).__init__()\n",
    "  \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(1, 1), dilation=(1, 1), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # 输出尺寸: (64, 16, 15)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5)  # 在全连接层前应用Dropout\n",
    "        )\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(15360, 2)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        self.model = nn.Sequential(self.block1, self.block2, self.fc)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CNN20DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 卷积块1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(3, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1))\n",
    "        )\n",
    "        \n",
    "        # 卷积块2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1))\n",
    "        )\n",
    "        \n",
    "        # 卷积块3（包含Flatten和Dropout）\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, (5, 3), padding=(3, 1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5)  # 全连接层前的Dropout\n",
    "        )\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(46080, 2)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.block1,\n",
    "            self.block2,\n",
    "            self.block3,\n",
    "            self.fc\n",
    "        )\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"应用Xavier初始化\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class CNN60DModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN60DModel, self).__init__()\n",
    "        \n",
    "        # 第一个 CNN 构建模块\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1), dilation=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # 高度减半至48，宽度保持180\n",
    "        )\n",
    "\n",
    "        # 第二个 CNN 构建模块\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # 高度减半至24，宽度保持180\n",
    "        )\n",
    "\n",
    "        # 第三个 CNN 构建模块\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # 高度减半至12，宽度保持180\n",
    "        )\n",
    "\n",
    "        # 第四个 CNN 构建模块（关键修改）\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 3), stride=(2, 3)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5)  # 全连接层前应用Dropout\n",
    "        )\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(184320, 2)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self.block1,\n",
    "            self.block2,\n",
    "            self.block3,\n",
    "            self.block4,\n",
    "            self.fc\n",
    "        )\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"应用Xavier初始化\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740273",
   "metadata": {},
   "source": [
    "## i5r5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaaa0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN5DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i5r5.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i5r5.pth'))\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i5r5 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i5r5_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_5']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i5r5.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i5r5.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i5r5.feather')\n",
    "test_df['pre_label_5'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i5r5.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56849661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i5r5.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_5'] == test_df['pre_label_5']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85007081",
   "metadata": {},
   "source": [
    "## i5r20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN5DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i5r20.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i5r20.pth'))\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i5r20 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i5r20_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_20']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i5r20.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i5r20.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i5r20.feather')\n",
    "test_df['pre_label_20'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i5r20.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i5r20.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_20'] == test_df['pre_label_20']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da8946",
   "metadata": {},
   "source": [
    "## i5r60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4c81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN5DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i5r60.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i5r60.pth'))\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i5r60 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i5r60_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_60']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i5r60.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i5r60.feather',\n",
    "    img_dir='./charts_train/5d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i5r60.feather')\n",
    "test_df['pre_label_60'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i5r60.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i5r60.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_60'] == test_df['pre_label_60']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b82d0",
   "metadata": {},
   "source": [
    "## i20r5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542332a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN20DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i20r5.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i20r5.pth'))\n",
    "                break\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i20r5 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i20r5_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_5']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i20r5.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i20r5.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i20r5.feather')\n",
    "test_df['pre_label_5'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i20r5.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i20r5.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_5'] == test_df['pre_label_5']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc27cf",
   "metadata": {},
   "source": [
    "## i20r20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN20DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i20r20.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i20r20.pth'))\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i20r20 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i20r20_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_20']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i20r20.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i20r20.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i20r20.feather')\n",
    "test_df['pre_label_20'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i20r20.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i20r20.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_20'] == test_df['pre_label_20']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604685f",
   "metadata": {},
   "source": [
    "## i20r60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN20DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=2):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i20r60.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i20r60.pth'))\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i20r60 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i20r60_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_60']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i20r60.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i20r60.feather',\n",
    "    img_dir='./charts_train/20d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i20r60.feather')\n",
    "test_df['pre_label_60'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i20r60.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5232f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i20r60.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_60'] == test_df['pre_label_60']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc8c5b",
   "metadata": {},
   "source": [
    "## i60r5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN60DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# 训练函数，包含早停机制\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=5):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i60r5.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i60r5.pth'))\n",
    "                break\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i60r5 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i60r5_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_5']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i60r5.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i60r5.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i60r5.feather')\n",
    "test_df['pre_label_5'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i60r5.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i60r5.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_5'] == test_df['pre_label_5']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186759d",
   "metadata": {},
   "source": [
    "## i60r20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7615a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN60DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=5):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i60r20.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i60r20.pth'))\n",
    "                break\n",
    "        \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i60r20 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i60r20_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_20']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i60r20.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i60r20.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i60r20.feather')\n",
    "test_df['pre_label_20'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i60r20.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i60r20.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_20'] == test_df['pre_label_20']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2076e",
   "metadata": {},
   "source": [
    "## i60r60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_optimizer(learning_rate=1e-5):  # 修改学习率为 1e-5\n",
    "    model = CNN60DModel()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return model, criterion, optimizer\n",
    "# 训练函数，包含早停机制\n",
    "\n",
    "def train_and_predict(model, train_loader, val_loader, criterion, optimizer, \n",
    "                     max_epochs=200, patience=5):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    predictions = {}\n",
    "    \n",
    "    # 用于记录每个 epoch 的 train_loss 和 val_loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{max_epochs}]')\n",
    "        \n",
    "        for batch_idx, (data, target, _) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'train_loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)  # 记录当前 epoch 的 train_loss\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target, paths in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # 保存预测结果\n",
    "                pred = output.argmax(dim=1).cpu().numpy()\n",
    "                for path, p in zip(paths, pred):\n",
    "                    predictions[path] = int(p)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)  # 记录当前 epoch 的 val_loss\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{max_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), './labels_train/best_model_i60r60.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                model.load_state_dict(torch.load('./labels_train/best_model_i60r60.pth'))\n",
    "                break\n",
    "       \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('i60r60 Train and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('./labels_train/i60r60_loss_curve.png')  # 保存图像为文件\n",
    "    plt.show()  # 显示图像\n",
    "    \n",
    "    return model, predictions\n",
    "\n",
    "class StockImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.labels_frame = pd.read_feather(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.labels_frame.iloc[idx]['image_path'].split('/')[-1])\n",
    "        # 直接读取为灰度图像，与生成函数保持一致\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels_frame.iloc[idx]['label_60']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, img_path\n",
    "    \n",
    "# 修改数据转换，保持与生成图像一致的尺寸\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化\n",
    "])\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/train_labels_i60r60.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = StockImageDataset(\n",
    "    csv_file='./labels_train/test_labels_i60r60.feather',\n",
    "    img_dir='./charts_train/60d_charts',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "model, criterion, optimizer = get_model_optimizer()\n",
    "trained_model, predictions = train_and_predict(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "# 将预测结果添加到标签文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_i60r60.feather')\n",
    "test_df['pre_label_60'] = test_df['image_path'].map(predictions)\n",
    "test_df.to_feather('./labels_train/test_labels_with_predictions_i60r60.feather')\n",
    "\n",
    "print(\"训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82106909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果文件\n",
    "test_df = pd.read_feather('./labels_train/test_labels_with_predictions_i60r60.feather')\n",
    "\n",
    "# 计算准确率\n",
    "correct_predictions = (test_df['label_60'] == test_df['pre_label_60']).sum()\n",
    "total_predictions = len(test_df)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
